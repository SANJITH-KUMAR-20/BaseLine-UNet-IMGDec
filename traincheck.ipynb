{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from Utils.Losses import Dice_Ceff, dice_loss\n",
    "from Utils.DataLoder import IMD2020Dataset\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from Utils.Eval import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.mkdir(\"checkpoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model , device, data_dir: str = None , epochs: int = 5,\n",
    "          batch_size : int = 32,\n",
    "          learning_rate: float = 1e-5,\n",
    "          save_checkpoints: bool = True,\n",
    "          isValidation: bool = True,\n",
    "          val_split: float = 0.1,\n",
    "          eval_step : int = 5,\n",
    "          weight_decay: float = 1e-8,\n",
    "          momentum: float = 0.999,\n",
    "          gradient_clipping: float = 1.0,\n",
    "          amp: bool = True ):\n",
    "\n",
    "    data = IMD2020Dataset(data_dir)\n",
    "\n",
    "    if isValidation:\n",
    "        dev_len = int(len(data)*val_split)\n",
    "        train_len = len(data) - dev_len\n",
    "\n",
    "        train_set, dev_set = random_split(data, [train_len,dev_len])\n",
    "        train_loader = DataLoader(train_set,batch_size,shuffle = True)\n",
    "        val_loader = DataLoader(dev_set, batch_size, False)\n",
    "\n",
    "    else:\n",
    "        train_len = len(data)\n",
    "        train_loader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer2 = torch.optim.AdamW(model.parameters(),lr = learning_rate, weight_decay= weight_decay)\n",
    "    optimizer1 = torch.optim.RMSprop(model.parameters(),lr=learning_rate,weight_decay=weight_decay,momentum=momentum)\n",
    "    optimizer3 = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay= weight_decay)\n",
    "    for optimizer in [optimizer1,optimizer2,optimizer3]:\n",
    "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\"max\",patience=4)\n",
    "      criterion = nn.BCEWithLogitsLoss()\n",
    "      dice = DiceLoss()\n",
    "      grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
    "\n",
    "      opt_folder_path = f\"/content/checkpoints/{type(optimizer).__name__}\"\n",
    "      if not os.path.exists(opt_folder_path):\n",
    "        os.mkdir(opt_folder_path)\n",
    "      global_step = 0\n",
    "      for epoch in range(1 , epochs + 1):\n",
    "          model.train()\n",
    "          loss = 0\n",
    "          accuracy = 0\n",
    "          dice_score = 0\n",
    "          epoch_loss = 0\n",
    "          with tqdm.tqdm(total = train_len, desc = f'Epoch {epoch} of {epochs}', unit = 'img') as pbar:\n",
    "\n",
    "              for batch in train_loader:\n",
    "\n",
    "\n",
    "                  fakes , masks = batch[0], batch[1]\n",
    "\n",
    "                  fakes = fakes.to(device = device, dtype = torch.float32)\n",
    "                  masks = masks.to(device = device, dtype = torch.long)\n",
    "                  with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):\n",
    "                    mask_pred = model(fakes)\n",
    "\n",
    "                    loss = criterion(mask_pred, masks.float())\n",
    "                    dice_loss = dice(F.sigmoid(mask_pred), masks.float())\n",
    "                    loss += dice_loss\n",
    "                    dice_score = 1 - dice_loss\n",
    "\n",
    "\n",
    "                    optimizer.zero_grad(set_to_none=True)\n",
    "                    grad_scaler.scale(loss).backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                    grad_scaler.step(optimizer)\n",
    "                    grad_scaler.update()\n",
    "\n",
    "\n",
    "\n",
    "                    pbar.update(fakes.shape[0])\n",
    "                    pbar.set_postfix(**{'loss (batch)': loss.item(), 'acc(batch)':2-loss.item(), 'dice_score(batch)':dice_score.item()})\n",
    "          epoch_loss += loss.item()\n",
    "          if isValidation and epoch % eval_step == 0:\n",
    "            val_loss,val_score = evaluate(model,device,val_loader,criterion,dev_len,True,eval_step,epoch,learning_rate)\n",
    "            scheduler.step(val_score)\n",
    "            pbar.set_description(desc= f\"Epoch-Loss : {epoch_loss}, Val_loss : {val_loss}, Val_Score : {val_score}\")\n",
    "          # else:\n",
    "          #   scheduler.step(1-epoch_loss)\n",
    "          #   pbar.set_description(desc= f\"Epoch-Loss : {epoch_loss}\")\n",
    "\n",
    "\n",
    "          if save_checkpoints:\n",
    "              mod_state_dict = model.state_dict()\n",
    "              opt_state_dict = optimizer.state_dict()\n",
    "              model_path = f\"/content/checkpoints/{type(optimizer).__name__}/checkpoint_epoch{epoch}.pt\"\n",
    "              metric_file_path = f\"/content/checkpoints/{type(optimizer).__name__}/Epoch_{epoch}.json\"\n",
    "              with open(metric_file_path, \"w\") as f:\n",
    "                try:\n",
    "                  data = {\"Epoch\" : epoch, \"Epoch_Loss\":epoch_loss,\n",
    "                          \"Val_loss\":val_loss, \"Val_Score\" : val_score, \"model_path\": model_path}\n",
    "                  json.dump(data,f,indent = 2)\n",
    "                except NameError:\n",
    "                  data = {\"Epoch\" : epoch, \"Epoch_Loss\":epoch_loss,\"model_path\": model_path}\n",
    "                  json.dump(data,f,indent = 2)\n",
    "              torch.save({\"model_state_dict\" : mod_state_dict,\n",
    "                          \"opt_state_dict\" : opt_state_dict}, model_path)\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.Basline_U_Net_Model import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (initial): ConvBlock(\n",
       "    (dconv): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (contract1): EncodeDown(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): ConvBlock(\n",
       "        (dconv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contract2): EncodeDown(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): ConvBlock(\n",
       "        (dconv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contract3): EncodeDown(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): ConvBlock(\n",
       "        (dconv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (contract4): EncodeDown(\n",
       "    (down): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): ConvBlock(\n",
       "        (dconv): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand1): DecodeUp(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (dconv): ConvBlock(\n",
       "      (dconv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand2): DecodeUp(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (dconv): ConvBlock(\n",
       "      (dconv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand3): DecodeUp(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (dconv): ConvBlock(\n",
       "      (dconv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (expand4): DecodeUp(\n",
       "    (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "    (dconv): ConvBlock(\n",
       "      (dconv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (resConv): ResultConv(\n",
       "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  train(model=model, device= device, data_dir=data_dir,save_checkpoints=True,epochs = 100)\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "  torch.cuda.empty_cache()\n",
    "  model.use_checkpointing()\n",
    "  train(model=model, device= device, data_dir=data_dir,save_checkpoints=True,epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
